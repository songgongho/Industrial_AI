# 02 - ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression) ğŸ¯

ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì™€ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ì„¸ ê°€ì§€ ë³€í˜•ì„ í•™ìŠµí•˜ëŠ” ì„¹ì…˜ì…ë‹ˆë‹¤.

## ğŸ“š í•™ìŠµ ëª©í‘œ

1. **ì´ì§„ ë¶„ë¥˜** ë¬¸ì œ ì´í•´
2. **ë¡œì§€ìŠ¤í‹± íšŒê·€** ì•Œê³ ë¦¬ì¦˜ ì›ë¦¬
3. **ê²½ì‚¬ í•˜ê°•ë²• ë¹„êµ**: Batch GD, SGD, Mini-batch GD
4. **í•˜ì´í¼íŒŒë¼ë¯¸í„°** ì¡°ì •ì˜ ì˜í–¥

---

## ğŸ“ íŒŒì¼ êµ¬ì„±

### 1. `logistic_regression_basic.ipynb`
**ë‚´ìš©**: ê¸°ë³¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸

**ëª¨ë¸ êµ¬ì¡°**:
```
ì…ë ¥ X
  â†“
ì„ í˜• ê²°í•©: z = wx + b
  â†“
Sigmoid: Ïƒ(z) = 1 / (1 + e^-z)
  â†“
í™•ë¥  â†’ ë¶„ë¥˜ (0 ë˜ëŠ” 1)
```

**Sigmoid í•¨ìˆ˜**:
```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# íŠ¹ì„±: Ïƒ(0) = 0.5, Ïƒ(-âˆ) = 0, Ïƒ(+âˆ) = 1
```

**ì†ì‹¤í•¨ìˆ˜ (Loss Function)**:
```
Binary Crossentropy:
L(y, Å·) = -[y*log(Å·) + (1-y)*log(1-Å·)]

í‰ê·  ì†ì‹¤ (Cross Entropy):
J = -(1/m) * Î£[y*log(Å·) + (1-y)*log(1-Å·)]
```

**ì„±ëŠ¥**:
```
ì •í™•ë„ (Accuracy): 85%+
AUC-ROC: 0.92
```

**ì½”ë“œ ì˜ˆì‹œ**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Dense(1, activation='sigmoid', input_shape=(input_dim,))
])

# ì»´íŒŒì¼
model.compile(
    optimizer='sgd',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# í›ˆë ¨
model.fit(X_train, y_train, epochs=100, batch_size=32)
```

---

### 2. `logistic_regression_gd_comparison.ipynb` â­ **í•µì‹¬ ì‹¤í—˜**

**ëª©í‘œ**: 3ê°€ì§€ ê²½ì‚¬ í•˜ê°•ë²• ë¹„êµ ë¶„ì„

**ë°ì´í„°ì…‹**: Make-moons (ë¹„ì„ í˜• ë¶„ë¥˜)
```python
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
```

**íŠ¹ì§•**:
- 2D ë°ì´í„° (ë‘ íŠ¹ì„±)
- ë¹„ì„ í˜• ê²½ê³„
- ì‹œê°í™” ìš©ì´

---

## ğŸ”¬ ê²½ì‚¬ í•˜ê°•ë²• ë¹„êµ

### ì„±ëŠ¥ ë¹„êµí‘œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ë°©ë²•       â”‚  ì •í™•ë„  â”‚   Loss   â”‚ ìˆ˜ë ´ì†ë„ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Batch GD        â”‚  84.5%   â”‚  0.38   â”‚   ëŠë¦¼   â”‚
â”‚ SGD             â”‚  86.5%   â”‚  0.32   â”‚   ë¹ ë¦„   â”‚
â”‚ Mini-batch GD   â”‚  85.0%   â”‚  0.35   â”‚   ì¤‘ê°„   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1ï¸âƒ£ Batch Gradient Descent (ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•)

**íŠ¹ì§•**:
```
ë§¤ ë°˜ë³µë§ˆë‹¤ ëª¨ë“  ìƒ˜í”Œë¡œ gradient ê³„ì‚°
â†’ ë§¤ìš° ì•ˆì •ì  ìˆ˜ë ´
â†’ ëŠë¦° ì—…ë°ì´íŠ¸ ì†ë„
```

**ì•Œê³ ë¦¬ì¦˜**:
```python
def batch_gd(X, y, epochs=100, learning_rate=0.1):
    m = X.shape[0]  # ìƒ˜í”Œ ìˆ˜
    
    for epoch in range(epochs):
        # ì „ì²´ ë°ì´í„°ë¡œ gradient ê³„ì‚°
        predictions = X @ weights + bias
        errors = predictions - y
        
        # ëª¨ë“  gradient í•©ì‚° (ë°°ì¹˜)
        weight_gradient = (X.T @ errors) / m
        bias_gradient = errors.mean()
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬ í›„)
        weights -= learning_rate * weight_gradient
        bias -= learning_rate * bias_gradient
        
        loss = mean_squared_error(y, predictions)
        print(f"Epoch {epoch+1}: Loss = {loss}")
```

**ì¥ì **:
- âœ… ë§¤ìš° ì•ˆì •ì ì¸ ìˆ˜ë ´ ê³¡ì„ 
- âœ… ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…ëœ ìˆ˜ë ´ì„±
- âœ… ìµœì ê°’ ê·¼ì²˜ì—ì„œ ì •í™•í•œ ì—…ë°ì´íŠ¸

**ë‹¨ì **:
- âŒ ë§¤ iterationë§ˆë‹¤ ì „ì²´ ë°ì´í„° í•„ìš” (ëŠë¦¼)
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ
- âŒ ì˜¨ë¼ì¸ í•™ìŠµ ë¶ˆê°€ëŠ¥

**ìˆ˜ë ´ ê³¡ì„ **:
```
Loss
  |     â•±â•²
  |   â•±    â•²â•±â•²
  | â•±         â•²â•±
  |_________ Epochs â†’
  
  íŠ¹ì§•: ë¶€ë“œëŸ½ê³  ë‹¨ì¡°ê°ì†Œ
```

---

### 2ï¸âƒ£ Stochastic Gradient Descent (í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)

**íŠ¹ì§•**:
```
ë§¤ ë°˜ë³µë§ˆë‹¤ 1ê°œ ìƒ˜í”Œë¡œë§Œ gradient ê³„ì‚°
â†’ ë§¤ìš° ë¹ ë¥¸ ì—…ë°ì´íŠ¸
â†’ ë¶ˆì•ˆì •í•œ ìˆ˜ë ´
```

**ì•Œê³ ë¦¬ì¦˜**:
```python
def sgd(X, y, epochs=100, learning_rate=0.1):
    m = X.shape[0]
    
    for epoch in range(epochs):
        for i in range(m):  # ê° ìƒ˜í”Œë§ˆë‹¤
            # ë‹¨ì¼ ìƒ˜í”Œë¡œ gradient ê³„ì‚°
            x_i = X[i:i+1]  # 1ê°œ ìƒ˜í”Œ
            y_i = y[i]
            
            prediction = x_i @ weights + bias
            error = prediction - y_i
            
            # ë‹¨ì¼ ìƒ˜í”Œ gradient
            weight_gradient = error * x_i
            bias_gradient = error
            
            # ì¦‰ì‹œ ì—…ë°ì´íŠ¸
            weights -= learning_rate * weight_gradient
            bias -= learning_rate * bias_gradient
```

**ì¥ì **:
- âœ… ë§¤ìš° ë¹ ë¥¸ ì´ˆê¸° ìˆ˜ë ´
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (1ê°œ ìƒ˜í”Œë§Œ)
- âœ… ì˜¨ë¼ì¸ í•™ìŠµ ê°€ëŠ¥

**ë‹¨ì **:
- âŒ ë¶ˆì•ˆì •í•œ ì§„ë™ ë§ìŒ
- âŒ ìˆ˜ë ´ í›„ ìµœì ê°’ ê·¼ì²˜ ì§„ë™
- âŒ ìµœì¢… ì •í™•ë„ ë¶ˆì•ˆì •

**ìˆ˜ë ´ ê³¡ì„ **:
```
Loss
  |   â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²
  | â•±              â•²
  |_________________ Epochs â†’
  
  íŠ¹ì§•: ì§€ê·¸ì¬ê·¸ íŒ¨í„´, ì´ˆê¸° ìˆ˜ë ´ ë¹ ë¦„
```

---

### 3ï¸âƒ£ Mini-batch Gradient Descent (ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•) â­ **ìµœì **

**íŠ¹ì§•**:
```
ë§¤ ë°˜ë³µë§ˆë‹¤ mê°œ ìƒ˜í”Œ (ë°°ì¹˜)ë¡œ gradient ê³„ì‚°
â†’ ì•ˆì •ì„±ê³¼ ì†ë„ì˜ ê· í˜•
â†’ ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©
```

**ì•Œê³ ë¦¬ì¦˜**:
```python
def mini_batch_gd(X, y, epochs=100, batch_size=32, learning_rate=0.1):
    m = X.shape[0]
    num_batches = m // batch_size
    
    for epoch in range(epochs):
        # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = start_idx + batch_size
            
            X_batch = X[start_idx:end_idx]
            y_batch = y[start_idx:end_idx]
            
            # ë¯¸ë‹ˆë°°ì¹˜ë¡œ gradient ê³„ì‚°
            predictions = X_batch @ weights + bias
            errors = predictions - y_batch
            
            weight_gradient = (X_batch.T @ errors) / batch_size
            bias_gradient = errors.mean()
            
            # ë°°ì¹˜ ë‹¨ìœ„ ì—…ë°ì´íŠ¸
            weights -= learning_rate * weight_gradient
            bias -= learning_rate * bias_gradient
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```
ë°°ì¹˜ í¬ê¸° ì„ íƒ:
- 32 (ê°€ì¥ ì¼ë°˜ì ) â† ì¶”ì²œ
- 64 (GPU í™œìš© ì‹œ)
- 128 (ëŒ€ê·œëª¨ ë°ì´í„°)

ë°°ì¹˜ê°€ ì‘ì„ìˆ˜ë¡:
- ì§„ë™ ì¦ê°€ (SGDì— ê°€ê¹Œì›€)
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

ë°°ì¹˜ê°€ í´ìˆ˜ë¡:
- ì•ˆì •ì„± ì¦ê°€ (Batch GDì— ê°€ê¹Œì›€)
- ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì 
```

**ì¥ì **:
- âœ… ì•ˆì •ì ì¸ ìˆ˜ë ´ (Batch GDì²˜ëŸ¼)
- âœ… ë¹ ë¥¸ ì—…ë°ì´íŠ¸ (SGDì²˜ëŸ¼)
- âœ… GPU/ë³‘ë ¬ ì²˜ë¦¬ í™œìš© ê°€ëŠ¥
- âœ… í•˜ë“œì›¨ì–´ ë©”ëª¨ë¦¬ ìµœì 

**ë‹¨ì **:
- âŒ ë°°ì¹˜ í¬ê¸° ì„ íƒ í•„ìš”

**ìˆ˜ë ´ ê³¡ì„ **:
```
Loss
  |   â•±â•±â”€â•±â”€â•±â”€â•±â”€
  | â•±â”€        â•±
  |_________ Epochs â†’
  
  íŠ¹ì§•: SGDë³´ë‹¤ ë¶€ë“œëŸ¬ì›€, Batch GDë³´ë‹¤ ë¹ ë¦„
```

---

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

### ì •í™•ë„ ë¹„êµ
```python
# ì„¸ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„
batch_accuracy = 84.5%    # ê°€ì¥ ë‚®ìŒ
minibatch_accuracy = 85.0% # ì¤‘ê°„
sgd_accuracy = 86.5%       # ê°€ì¥ ë†’ìŒ (í•˜ì§€ë§Œ ë¶ˆì•ˆì •)

# í›ˆë ¨ ê³¡ì„ ì—ì„œì˜ í‘œì¤€í¸ì°¨
batch_std = 0.5%      # ê°€ì¥ ì•ˆì •ì 
minibatch_std = 1.2%  # ì¤‘ê°„
sgd_std = 3.5%        # ê°€ì¥ ë¶ˆì•ˆì •
```

### ìˆ˜ë ´ ì†ë„ (Learning Curve)
```
Epochë³„ Loss ê°’:
Epoch 1:  0.85 (Batch) | 0.72 (SGD) | 0.78 (Mini)
Epoch 5:  0.58 (Batch) | 0.38 (SGD) | 0.42 (Mini)
Epoch 20: 0.39 (Batch) | 0.35 (SGD) | 0.35 (Mini)

â†’ SGD: ì´ˆê¸° ìˆ˜ë ´ ë¹ ë¦„, ìµœì¢… ì§„ë™
â†’ Mini-batch: ì¤‘ê°„ ì†ë„, ì•ˆì •ì 
â†’ Batch: ëŠë¦° ìˆ˜ë ´, ë§¤ìš° ì•ˆì •ì 
```

---

## ğŸ’¡ ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

### ìƒí™©ë³„ ìµœì  ì„ íƒ

```
1. ì†Œê·œëª¨ ë°ì´í„° (< 10,000)
   â†’ Batch GD ì¶”ì²œ
   ì´ìœ : ê³„ì‚°ëŸ‰ ì ìŒ, ì•ˆì •ì„± ì¤‘ìš”

2. ë§¤ìš° í° ë°ì´í„° (> 1,000,000)
   â†’ SGD ë˜ëŠ” Mini-batch (ì‘ì€ í¬ê¸°)
   ì´ìœ : ë©”ëª¨ë¦¬ ì œì•½, ë¹ ë¥¸ ìˆ˜ë ´ í•„ìš”

3. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° (ì¶”ì²œ) â­
   â†’ Mini-batch GD (í¬ê¸°: 32-128)
   ì´ìœ : ìµœì ì˜ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±

4. ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°
   â†’ SGD (ì˜¨ë¼ì¸ í•™ìŠµ)
   ì´ìœ : ë°ì´í„° ì§€ì†ì  ë“¤ì–´ì˜´

5. GPU ë³‘ë ¬ ì²˜ë¦¬
   â†’ Mini-batch (í¬ê¸°: 64-256)
   ì´ìœ : GPU í™œìš© ìµœì í™”
```

---

## ğŸ”§ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
pip install tensorflow scikit-learn numpy matplotlib
```

### 2. Jupyter ì‹¤í–‰
```bash
jupyter notebook
```

### 3. ì‹¤ìŠµ ê³¼ì •
1. `logistic_regression_basic.ipynb` ì‹¤í–‰
2. `logistic_regression_gd_comparison.ipynb` ì‹¤í–‰
3. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ë¶„ì„

---

## ğŸ“Œ í•µì‹¬ êµí›ˆ

1. **ê²½ì‚¬ í•˜ê°•ë²• ì„ íƒì€ ì¤‘ìš”í•¨**
   - ì„±ëŠ¥: SGD > Mini-batch > Batch (ë¶ˆì•ˆì •ì„± í¬í•¨)
   - ì•ˆì •ì„±: Batch > Mini-batch > SGD
   - ì†ë„: SGD > Mini-batch > Batch

2. **ë°°ì¹˜ í¬ê¸°ì˜ ì˜í–¥**
   - ì‘ì€ ë°°ì¹˜: ë¹ ë¥´ì§€ë§Œ ì§„ë™
   - í° ë°°ì¹˜: ëŠë¦¬ì§€ë§Œ ì•ˆì •ì 
   - ìµœì : 32-128 ë²”ìœ„

3. **ì‹¤ë¬´ì—ì„œì˜ ì„ íƒ**
   - ëŒ€ë¶€ë¶„ Mini-batch GD ì‚¬ìš©
   - ë°°ì¹˜ í¬ê¸°: 32 (ê¸°ë³¸ê°’)
   - ëŸ¬ë‹ ë ˆì´íŠ¸: 0.001~0.01

---

## ğŸ”— ë‹¤ìŒ ë‹¨ê³„

â†’ **ì‹ ê²½ë§** (ë” ë³µì¡í•œ ë¶„ë¥˜)  
â†’ **ìµœì í™” ì•Œê³ ë¦¬ì¦˜** (Adam, RMSprop)  
â†’ **ì •ê·œí™”** (L1, L2)

